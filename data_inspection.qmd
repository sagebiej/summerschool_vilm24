---
title: "Breakout Session Choice Experiments"
author: "Julian Sagebiel"
date: "2024-07-12"
format: 
  html:
    self-contained: true 
    page-layout: full
    toc: true
    toc-depth: 2
    css: styles.css
    code-fold: true
    code-tools: true
    theme: 
      light: "flatly"
      dark: "darkly"
highlight-style: github
---

# Read data and inspect



The whole project can be downloaded here https://cloud.idiv.de/nextcloud/index.php/s/8xXJexMYg5KGasY or here https://tinyurl.com/bschoice24 

Make sure you have installed the following packages. The package choiceTools is only available on gitlab and can be installed if you copy the following code into your R console 

`remotes::install_gitlab("dj44vuri/choicetools", host = "https://git.idiv.de")`


In this case, the data is stored in an RDS file.

```{r, message=FALSE}


rm(list=ls())
library(kableExtra)
library(dplyr)
library(choiceTools)
library(ggplot2)
library(purrr)
library(apollo)
library(texreg)



data <- readRDS("data/simulateddata_n10000.RDS")[["des1"]][[1]] %>% 
  select(-c("Split", "group","logdist","alt2_sq" , "HNV_SQsq", "HNV_sharesq", starts_with(c("V","e_", "U_"))))

```

We first look at the first 40 rows of the dataset.

```{r}

kable(data[1:40,], digits = 2) %>% kable_styling()



```

```{r}

descriptions <- c("Respndent identifier", "Identifier for each choice situation", paste0(1:5,". attribute in alternative 1"),
                  "Unique identifier of the Block (if blocks are present)" , 
                  "Longitude of respondent's home", "latitude of respondent's home", "Radius where the changes take place", 
                  "Respondents Status Quo of NHV", 
                  "Respondents Status Quo of protected areas", 
                  paste0("Share of ",c("HNV","protected area"), " in respondent's area"), 
                  paste0("Absolute value of protected area adapted to respondent's status quo in alternative ",1:2),
                  paste0("Absolute value of HNV adapted to respondent's status quo in alternative ",1:2),
                  paste0("Square of absolute value of protected area adapted to respondents status quo in alternative ",1:2),
                  paste0("Square of absolute value of HNV adapted to respondent's status quo in alternative ",1:2), 
                  "Chosen Alternative"
                  
                  
                  )

description_table <- data.frame(
  ColumnName = names(data),
  Description = descriptions
)



kable(description_table) %>% kable_styling()
```

Things to note:

-   Each respondent replied to `r length(data$ID[data$ID==1])` choice sets.
-   Each row is one choice set
-   Each respondent has a unique status quo

```{r}


# Create the data for the table
attributes <- data.frame(
  Column1 = c("x1", "x2", "x3", "x4", "x5"),
  Column2 = c("High nature value farmland", "Visibility", "Protected Area", "Accessibility", "Payment"),
  Column3 = c(
    "Extention of High Nature Value areas in ha",
    "Visibility of the area, considering how well it can be seen from roads an paths",
    "Extention of protected areas in ha",
    "To what extent the area is accessable",
    "The yearly contribution by an individual"
  ),
  Column4 = c("1 = 100 ha etc.", "0 = not visible, 1 = visible", "1 = 100 ha etc." , "0= not accessable, 1 = half of new area accessable, 2 = full new area accessable", "0.5 =50 Euro etc.")
)

# Create the table using kable
kable(attributes, col.names = c("Variable", "Short description", "Description", "Levels"), 
      caption = "Sample Table") %>% kable_styling()

```

Before we start estimating models, we should descriptively look at the data.

One important graph is the bid curve. It tells us the (relative) frequencies of chosen alternatives given the price level

```{r}


summary <- data %>% 
  select(alt1_x5,CHOICE) %>% 
  group_by(alt1_x5) %>% 
    summarize(
    count = sum(CHOICE == 1),
    total = n(),
    proportion = count / total
  )


kable(summary) %>% kable_styling()


ggplot(summary, aes(x=alt1_x5, y=proportion)) +
  geom_line()

```


We may want to repeat this with other attributes. To avoid repeating code, we write a function and loop it over all attributes.

```{r}
relative_freq <- function(group_var) {
  data %>%
    select({{ group_var }}, CHOICE) %>%
    group_by({{ group_var }}) %>%
    summarize(
      count = sum(CHOICE == 1),
      total = n(),
      proportion = count / total
    )
}


relative_freq(alt1_x1)

relative_freq(alt1_x2)


```

To repeat the function over all attributes, we could use a loop (in this case `map` from the `purrr` package).

```{r}

attr <- names(data %>% select(starts_with("alt1_x")))  # identify attributes

map(attr,~relative_freq(!!sym(.x)))                    # loop over attributes

```

# Model estimation


The first model we estimate follows the following linear utility function

$$U_1= \beta_{hnv}*alt1\_x1+\beta_{vis}*alt1_x2 + \beta_{prot}*alt1\_x3 + \beta_{acc}*alt1\_x4 + \beta_{cost} * alt1\_x5   $$
And for the Status Quo, we just add a constant

$$U_{SQ} = \beta_{SQ}$$

We estimate the model with the `apollo` package and the code is stored in `"Code/c_logit_linear.R"`

We can open this file to look into it. To estimate the model, we source it here.

We store the model as an RDS file, so we do not need to estimate it anytime we run the code.


```{r, eval=FALSE}
source("Code/c_logit_model_linear.R")
```

Once we have estimated and stored the model, we can quickly load it.
```{r, message=FALSE}

# Load model
model_linear <- apollo_loadModel("Estimation_results/Clogit_linear")

model_linear_tex <- quicktexregapollo(model_linear)
```

And look into model results.



```{r, results='asis'}
htmlreg(model_linear_tex,single.row = TRUE,digits = 2)
```

We can also look into willingness to pay by applying the WTP formula

$$ WTP_a = \cfrac{\partial U/\partial a}{\partial U / \partial_c} $$







```{r}
deltaMethod_settings=list(expression=c(WTP_HNV="-10*mu_hnv/(mu_cost)",
                                       WTP_Vis="-10*mu_hnv_vis/(mu_cost)"
                                       ))
WTP= apollo_deltaMethod(model_linear, deltaMethod_settings)
```

In this model, a 100 ha increase translates to a willingness to pay of `r WTP$Value[1]*100`. If the area is visible, willingness to pay is `r WTP$Value[1]*100`. This is not very realistic. The willingness to pay for visibility may depend on the total increase in HNV. And the willingness to pay for 1ha more HNV may not be constant.



We could also use the actual ha values in the model. This is  `r "Code/c_logit_model_linear_sq.R"`


```{r, eval=FALSE, include=FALSE}
source("Code/c_logit_model_linear_sq.R")
```


```{r, message=FALSE, include=FALSE}

# Load model
model_linearsq <- apollo_loadModel("Estimation_results/Clogit_linear_sq")

model_linearsq_tex <- quicktexregapollo(model_linearsq)
```

And look into model results.



```{r, results='asis'}
htmlreg(list(model_linear_tex, model_linearsq_tex),single.row = TRUE,digits = 2)
```

### Non Linear Specification


Now lets assume that the marginal utility of one more unit HNV and protected area is non-linear.

As a first try, we assume a quadratic relationship. 

The utility function is 

 $$ U_1= \beta_{hnv}*alt1\_HNV+ \beta_{hnv2} *alt1\_HNVsq + \beta_{vis}*alt1\_x2 + \beta_{prot}*alt1\_protected  + \beta_{prot2}*alt1\_protectedsq  + \beta_{acc}*x4 + \beta_{cost} * x5    $$
 
 
 
 and the utility for the status quo is 
 
 $$ U_2= \beta_{sq} + \beta_{hnv}*alt2\_HNV+ + \beta_{hnv2} *alt2\_HNVsq + \beta_{prot}*alt2\_protected  + \beta_{prot2}*alt2\_protectedsq $$ 


```{r, eval=FALSE, message=FALSE}
source("Code/c_logit_model_gp.R")
```





Load estimated model if you don't want to reestimate the model.

```{r, message=FALSE}

# Load model
model_gp <- apollo_loadModel("Estimation_results/Clogit GP")
```

Take a look at the estimated model.

```{r}
apollo_modelOutput(model_gp)
```

Plot utility function for HNV.

```{r}
u_hnv <- function(x, y) {y$estimate["mu_hnv"]*x + y$estimate["mu_hnv2"]*x^2} # Use coefficient names of estimated model

data_vector <- c(0:350)

data_vec_utility <- u_hnv(data_vector, model_gp)

ggplot() +
  geom_line(aes(x=data_vector, y=data_vec_utility), col="peru") +
  xlab("Status Quo HNV in 100ha") +
  ylab("Utility")

```

Calculate WTP from estimated model.

```{r}
###### Calculate WTP for HNV ####



wtp_hnv <- function(x, y) {100*-((y$estimate[["mu_hnv"]] + 2*y$estimate["mu_hnv2"]*x)/y$estimate["mu_cost"])}


```

Plot WTP function for HNV.

```{r}

data_vec_wtp <- wtp_hnv(data_vector, model_gp)


ggplot() +
  geom_line(aes(x=data_vector, y=data_vec_wtp), col="peru") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") + 
  xlab("Status Quo HNV in 100ha") +
  ylab("WTP (€/100ha)")
```

# Create Maps

```{r, message=FALSE}
# Load GIS packages 
library(ggspatial)
library(viridis)
library(sf)
library(terra)
```

Load in GIS county data

```{r}
germany_county <- read_sf("GIS/Data/prepro/gadm41_DEU_shp", "gadm41_DEU_2") # county borders

counties_gis <- readRDS("counties_gis.rds")

germany_county <- st_transform(germany_county, st_crs(counties_gis$geometry)) # Harmonize crs's
```

Plot WTP for 100ha increase in HNV for all German counties.

```{r}
counties_wtp <-counties_gis %>% mutate(WTP_HNV = wtp_hnv(Average_HNV_SQ/100, model_gp)) # WTP in €/100ha


# Plot HNV WTP distribution
ggplot(data=germany_county) + 
  geom_sf(fill="gray98", alpha=1) +
  geom_sf(data=counties_wtp, aes(fill=WTP_HNV, geometry=geometry), size=2) +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  labs(fill= "WTP(€/100ha) HNV") +
  scale_fill_gradientn(colours = c("white", "cornsilk1", "peru")) +
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"))
```

```{r}
###### Calculate WTP for Protected Areas ####

wtp_pa <- function(x, y) {100*-((y$estimate[["mu_pa"]] + 2*y$estimate["mu_pa2"]*x)/y$estimate["mu_cost"])}

counties_wtp <-counties_gis %>% mutate(WTP_Prot = wtp_pa(Average_Prot_SQ/100, model_gp)) # WTP in €/100ha
```

```{r}
# Plot Prot WTP distribution
ggplot(data=germany_county) + 
  geom_sf(fill="gray98", alpha=1) +
  geom_sf(data=counties_wtp, aes(fill=WTP_Prot, geometry=geometry), size=2) +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  labs(fill= "WTP(€/100ha) Protected Areas") +
  scale_fill_gradientn(colours = c("white", "cornsilk1", "forestgreen")) +
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"))
```

Mapping the aggregated WTP per 5x5km raster cells

Procedure:

1\. Calculate status quo of HNV for 5x5km cells.

2\. Calculate marginal WTP for each raster cell using model estimates.

3\. Match population density data and compute WTP per cell.

Here we simply read in the preprocessed raster with the aggregated WTP values and plot it.

```{r}
wtp_raster_agg <- rast("hnv_wtp.tif") # Read in preprocessed raster with WTP values

wtp_hnv_gg <- as.data.frame(wtp_raster_agg, xy=TRUE)

wtp_hnv_gg$mean <- wtp_hnv_gg$mean/1000000

wtp_hnv_gg$cuts <- cut(wtp_hnv_gg$mean, breaks = c(-2, 0, 0.5, 1, 2, 5, 8, 12, 20, 30, 50))

ggplot(data=wtp_hnv_gg) +
  geom_tile(aes(x = x, y = y, fill = cuts)) +
  coord_equal() +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  scale_fill_viridis(option = "F", discrete=TRUE, 
                     labels=c("< 0", "0 - 0.5", "0.5 - 1", "1 - 2", "2 - 5", "5 - 8", "8 - 12",
                              "12 - 20", "20 - 30", "> 30")) +
  xlab("") +
  ylab("") +
  labs(fill ="Aggregated WTP \n(Mio. €/100ha) HNV") +
  theme(legend.position = "right")
```
