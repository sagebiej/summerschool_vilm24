---
title: "Data Inspection and Analysis"
author: "Julian Sagebiel"
date: "2024-07-12"
format: 
  html:
    page-layout: full
    toc: true
    toc-depth: 2
    css: styles.css
    code-fold: true
    code-tools: true
    theme: 
      light: "flatly"
      dark: "darkly"
highlight-style: github
---

# Read data and inspect

The data is somehow hidden in an RDS file.

```{r, message=FALSE}
rm(list=ls())
library(kableExtra)
library(dplyr)
library(choiceTools)
library(ggplot2)
library(purrr)
library(apollo)
library(texreg)



data <- readRDS("simulateddata_n10000.RDS")[["des1"]][[1]] %>% 
  select(-c("Split", "group","logdist", "HNV_SQsq", "HNV_sharesq", starts_with(c("V","e_", "U_"))))

```

We first look at the first 100 rows of the dataset

```{r}

kable(data[1:100,], digits = 2) %>% kable_styling()



```

```{r}

descriptions <- c("Respndent identifier", "Identifier for each choice situation", paste0(1:5,". attribute in alternative 1"),
                  "Alternative Specific Constant for Status Quo", "Unique identifier of the Block (if blocks are present)" , 
                  "Longitude of respondent's home", "latitude of respondent's home", "Radius where the changes take place", 
                  "Respondents Status Quo of NHV", 
                  "Respondents Status Quo of protected areas", 
                  paste0("Share of ",c("HNV","protected area"), " in respondent's area"), 
                  paste0("Absolute value of protected area adapted to respondent's status quo in alternative ",1:2),
                  paste0("Absolute value of HNV adapted to respondent's status quo in alternative ",1:2),
                  paste0("Square of absolute value of protected area adapted to respondents status quo in alternative ",1:2),
                  paste0("Square of absolute value of HNV adapted to respondent's status quo in alternative ",1:2), 
                  "Chosen Alternative"
                  
                  
                  )

description_table <- data.frame(
  ColumnName = names(data),
  Description = descriptions
)



kable(description_table) %>% kable_styling()
```

Things to note:

-   Each respondent replied to `r length(data$ID[data$ID==1])` choice sets.
-   Each row is one choice set
-   Each respondent has a unique status quo

```{r}


# Create the data for the table
attributes <- data.frame(
  Column1 = c("x1", "x2", "x3", "x4", "x5"),
  Column2 = c("High nature value farmland", "Visibility", "Protected Area", "Accessibility", "Payment"),
  Column3 = c(
    "Extention of High Nature Value areas in ha.",
    "Visibility of the area, considering how well it can be seen from roads an paths.",
    "Extention of protected areas in ha.",
    "To what extent the area is accessable",
    "The yearly contribution by an individual"
  ),
  Column4 = c("1 = 100 ha etc.", "0 = not visible, 1 = visible", "1 = 100 ha etc." , "0= not accessable, 1 = half of new area accessable, 2 = full new area accessable", "0.5 =50 Euro etc.")
)

# Create the table using kable
kable(attributes, col.names = c("Variable", "Short description", "Description", "Levels"), 
      caption = "Sample Table") %>% kable_styling()

```

Before we start estimating models, we should always first descriptively look at the data.

One important graph is the bid curve. It tells us the frequencies of chosen alternatives given the price level

```{r}


summary <- data %>% 
  select(alt1_x5,CHOICE) %>% 
  group_by(alt1_x5) %>% 
    summarize(
    count = sum(CHOICE == 1),
    total = n(),
    proportion = count / total
  )


kable(summary) %>% kable_styling()


ggplot(summary, aes(x=alt1_x5, y=proportion)) +
  geom_line()

```

You can write a function to automate this

```{r}
relative_freq <- function(group_var) {
  data %>%
    select({{ group_var }}, CHOICE) %>%
    group_by({{ group_var }}) %>%
    summarize(
      count = sum(CHOICE == 1),
      total = n(),
      proportion = count / total
    )
}


relative_freq(alt1_x1)

relative_freq(alt1_x2)


```

To automate that more, you could use a loop

```{r}

attr <- names(data %>% select(starts_with("alt1_x")))

map(attr,~relative_freq(!!sym(.x)))

```

# Model estimation

Once we think we understood the data, we will prepare the data 

```{r}
# Create Dummys for Accessibility of Protected areas

data <- data %>% mutate(Dummy_Half = case_when(alt1_x4 == 1 ~ 1, TRUE~0),
                        Dummy_Full = case_when(alt1_x4 == 2 ~ 1, TRUE~0))
```


The first model we estimate follows the following linear utility function

$$U_1= \beta_{hnv}*x1+\beta_{vis}*x2 + \beta_{prot}*x3 + \beta_{acc}*x4 + \beta_{cost} * x5   $$
And for the Status Quo, we just add a constant

$$U_{SQ} = \beta_{SQ}$$

We estimate the model with the `apollo` package and the code is stored in `"Code/c_logit_linear.R"`

We can open this file to look into it. To estimate the model, we source it here.

We store the model as an RDS file, so we do not need to estimate it anytime we run the code.


```{r, eval=FALSE}
source("Code/c_logit_linear.R")
```

Once we have estimated and stored the model, we can quickly load it.
```{r, message=FALSE}

# Load model
model_linear <- apollo_loadModel("Estimation_results/Clogit_linear")

model_linear_tex <- quicktexregapollo(model_linear)
```

And look into model results.



```{r, results='asis'}
htmlreg(model_linear,single.row = TRUE,digits = 2)
```

We can also look into willingness to pay by applying the WTP formula

$$ WTP_a = \cfrac{\partial U/\partial a}{\partial U / \partial_c} $$






Estimate model with utility function incorporating non-linear utility. 




```{r, eval=FALSE, message=FALSE}
source("Code/c_logit_model_gp.R")
```

Load estimated model if you don't want to reestimate the model.

```{r, message=FALSE}
library(apollo)
# Load model
model_gp <- apollo_loadModel("Estimation_results/Clogit GP")
```

Take a look at the estimated model.

```{r}
apollo_modelOutput(model_gp)
```

Plot utility function for HNV.

```{r}
u_hnv <- function(x, y) {y$estimate["mu_hnv"]*x + y$estimate["mu_hnv2"]*x^2} # Use coefficient names of estimated model

data_vector <- c(0:350)

data_vec_utility <- u_hnv(data_vector, model_gp)

ggplot() +
  geom_line(aes(x=data_vector, y=data_vec_utility), col="peru") +
  xlab("Status Quo HNV in 100ha") +
  ylab("Utility")

```

Calculate WTP from estimated model.

```{r}
###### Calculate WTP for HNV ####

model_gp$estimate

wtp_hnv <- function(x, y) {100*-((y$estimate[["mu_hnv"]] + 2*y$estimate["mu_hnv2"]*x)/y$estimate["mu_cost"])}


```

Plot WTP function for HNV.

```{r}

data_vec_wtp <- wtp_hnv(data_vector, model_gp)


ggplot() +
  geom_line(aes(x=data_vector, y=data_vec_wtp), col="peru") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") + 
  xlab("Status Quo HNV in 100ha") +
  ylab("WTP (€/100ha)")
```

# Create Maps

```{r, message=FALSE}
# Load GIS packages 
library(ggspatial)
library(viridis)
library(sf)
library(terra)
```

Load in GIS county data

```{r}
germany_county <- read_sf("GIS/Data/prepro/gadm41_DEU_shp", "gadm41_DEU_2") # county borders

counties_gis <- readRDS("counties_gis.rds")

germany_county <- st_transform(germany_county, st_crs(counties_gis$geometry)) # Harmonize crs's
```

Plot WTP for 100ha increase in HNV for all German counties.

```{r}
counties_wtp <-counties_gis %>% mutate(WTP_HNV = wtp_hnv(Average_HNV_SQ/100, model_gp)) # WTP in €/100ha


# Plot HNV WTP distribution
ggplot(data=germany_county) + 
  geom_sf(fill="gray98", alpha=1) +
  geom_sf(data=counties_wtp, aes(fill=WTP_HNV, geometry=geometry), size=2) +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  labs(fill= "WTP(€/100ha) HNV") +
  scale_fill_gradientn(colours = c("white", "cornsilk1", "peru")) +
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"))
```

```{r}
###### Calculate WTP for Protected Areas ####

wtp_pa <- function(x, y) {100*-((y$estimate[["mu_pa"]] + 2*y$estimate["mu_pa2"]*x)/y$estimate["mu_cost"])}

counties_wtp <-counties_gis %>% mutate(WTP_Prot = wtp_pa(Average_Prot_SQ/100, model_gp)) # WTP in €/100ha
```

```{r}
# Plot Prot WTP distribution
ggplot(data=germany_county) + 
  geom_sf(fill="gray98", alpha=1) +
  geom_sf(data=counties_wtp, aes(fill=WTP_Prot, geometry=geometry), size=2) +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  labs(fill= "WTP(€/100ha) Protected Areas") +
  scale_fill_gradientn(colours = c("white", "cornsilk1", "forestgreen")) +
  theme(plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"))
```

Mapping the aggregated WTP per 5x5km raster cells

Procedure:

1\. Calculate status quo of HNV for 5x5km cells.

2\. Calculate marginal WTP for each raster cell using model estimates.

3\. Match population density data and compute WTP per cell.

Here we simply read in the preprocessed raster with the aggregated WTP values and plot it.

```{r}
wtp_raster_agg <- rast("hnv_wtp.tif") # Read in preprocessed raster with WTP values

wtp_hnv_gg <- as.data.frame(wtp_raster_agg, xy=TRUE)

wtp_hnv_gg$mean <- wtp_hnv_gg$mean/1000000

wtp_hnv_gg$cuts <- cut(wtp_hnv_gg$mean, breaks = c(-2, 0, 0.5, 1, 2, 5, 8, 12, 20, 30, 50))

ggplot(data=wtp_hnv_gg) +
  geom_tile(aes(x = x, y = y, fill = cuts)) +
  coord_equal() +
  annotation_scale(location = "bl", width_hint = 0.1,plot_unit="m", line_width = 0.1) +
  annotation_north_arrow(location = "br" ,pad_x = unit(0, "in"), pad_y = unit(0, "in"),
                         style = north_arrow_fancy_orienteering) +
  scale_fill_viridis(option = "F", discrete=TRUE, 
                     labels=c("< 0", "0 - 0.5", "0.5 - 1", "1 - 2", "2 - 5", "5 - 8", "8 - 12",
                              "12 - 20", "20 - 30", "> 30")) +
  xlab("") +
  ylab("") +
  labs(fill ="Aggregated WTP \n(Mio. €/100ha) HNV") +
  theme(legend.position = "right")
```
